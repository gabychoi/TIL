{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAC에 spark 설치 요망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD는 두 가지 연산으로 이뤄져 있다.\n",
    "- Transformation \n",
    "- Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformation -> Lazy Execution 또는 Lazy Loading\n",
    "- 트랜스포메이션이 행해지면, RDD가 수행되는 것이 아닌, 새로운 RDD를 만들어 내고 \n",
    "- 그 새로운 RDD에 수행 결과를 저장하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Action\n",
    "- 메서드로 이루어진 실행 작업이며, 트랜스포메이션이 행해지고 나서 이뤄지는 Evaluation 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf , SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-CS5B0E0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=sparkApp>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setMaster('local').setAppName('sparkApp')\n",
    "spark = SparkContext(conf=conf)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/Users/HojinChoi/Desktop/data/test.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.textFile('/Users/HojinChoi/Desktop/data/test.txt')\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = rdd.filter(lambda x : 'spark' in x)\n",
    "lines "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- RDD 생성\n",
    "- 데이터를 직접 만드는 방법(parallielize()) , 외부 데이터를 로드 방법으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rdd = spark.parallelize(['test' , 'this is a test rdd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD 자주 쓰는 연산 함수\n",
    "- collect() : RDD에 트랜스포메이션된 결과를 리턴하는 함수\n",
    "- map() : 연산 수행하고 싶을 때 사용하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[5] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = spark.parallelize(list(range(5)))\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = numbers.map(lambda x : x * x).collect()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- flatmap() : 리스트들의 원소를 하나의 리스트로 flatten 해서 리턴하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'spark', 'hi', 'python', 'hi', 'django', 'hi', 'sklearn']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = spark.parallelize(['hi spark' , 'hi python' , 'hi django' , 'hi sklearn'])\n",
    "flat_string = strings.flatMap(lambda x : x.split(' ')).collect()\n",
    "flat_string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter() : 조건으로 필터링하는 연산자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[15] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = spark.parallelize(list(range(1, 30, 3)))\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 10, 16, 22, 28]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = num.filter(lambda x : x % 2 == 0).collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDD\n",
    "- pair rdd 란 key-value 쌍으로 이뤄진 RDD\n",
    "- python tuple 을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[17] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair rdd 생성\n",
    "pairRDD = spark.parallelize([(1, 3) , (1, 5) , (2, 4) , (3 , 3)])\n",
    "pairRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 8, 2: 4, 3: 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    i : j\n",
    "    for i , j in pairRDD.reduceByKey(lambda x, y : x+y).collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\i\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\i\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-1daaddfdf63f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m {\n\u001b[0;32m      2\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpairRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m }\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    814\u001b[0m         \"\"\"\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    817\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\i\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Users\\i\\Anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\i\\Anaconda3\\lib\\site-packages\\pyspark\\rdd.py\", line 1983, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\nTypeError: <lambda>() missing 1 required positional argument: 'y'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:990)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "{\n",
    "    i : j\n",
    "    for i , j in pairRDD.mapValues(lambda x, y : x**2).collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 4, 3]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 5), (2, 4), (3, 3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairRDD.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 외부 데이터를 로드해서 RDD 생성하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C:/Users/i/TIL/Spark/exec_data/spark-rdd-name-customers.csv MapPartitionsRDD[28] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerRDD = spark.textFile('C:/Users/i/TIL/Spark/exec_data/spark-rdd-name-customers.csv')\n",
    "customerRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alfreds Futterkiste,Germany'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alfreds Futterkiste', 'Germany'),\n",
       " ('Ana Trujillo Emparedados y helados', 'Mexico'),\n",
       " ('Antonio Moreno Taqueria', 'Mexico'),\n",
       " ('Around the Horn', 'UK'),\n",
       " ('Berglunds snabbkop', 'Sweden'),\n",
       " ('Blauer See Delikatessen', 'Germany'),\n",
       " ('Blondel pere et fils', 'France'),\n",
       " ('Bolido Comidas preparadas', 'Spain'),\n",
       " (\"Bon app'\", 'France'),\n",
       " ('Bottom-Dollar Marketse', 'Canada'),\n",
       " (\"B's Beverages\", 'UK'),\n",
       " ('Cactus Comidas para llevar', 'Argentina'),\n",
       " ('Centro comercial Moctezuma', 'Mexico'),\n",
       " ('Chop-suey Chinese', 'Switzerland'),\n",
       " ('Comercio Mineiro', 'Brazil'),\n",
       " ('Consolidated Holdings', 'UK'),\n",
       " ('Drachenblut Delikatessend', 'Germany'),\n",
       " ('Du monde entier', 'France'),\n",
       " ('Eastern Connection', 'UK'),\n",
       " ('Ernst Handel', 'Austria'),\n",
       " ('Familia Arquibaldo', 'Brazil'),\n",
       " ('FISSA Fabrica Inter. Salchichas S.A.', 'Spain'),\n",
       " ('Folies gourmandes', 'France'),\n",
       " ('Folk och fa HB', 'Sweden'),\n",
       " ('Frankenversand', 'Germany'),\n",
       " ('France restauration', 'France'),\n",
       " ('Franchi S.p.A.', 'Italy'),\n",
       " ('Furia Bacalhau e Frutos do Mar', 'Portugal'),\n",
       " ('Galeria del gastronomo', 'Spain'),\n",
       " ('Godos Cocina Tipica', 'Spain'),\n",
       " ('Gourmet Lanchonetes', 'Brazil'),\n",
       " ('Great Lakes Food Market', 'USA'),\n",
       " ('GROSELLA-Restaurante', 'Venezuela'),\n",
       " ('Hanari Carnes', 'Brazil'),\n",
       " ('HILARIoN-Abastos', 'Venezuela'),\n",
       " ('Hungry Coyote Import Store', 'USA'),\n",
       " ('Hungry Owl All-Night Grocers', 'Ireland'),\n",
       " ('Island Trading', 'UK'),\n",
       " ('Koniglich Essen', 'Germany'),\n",
       " (\"La corne d'abondance\", 'France'),\n",
       " (\"La maison d'Asie\", 'France'),\n",
       " ('Laughing Bacchus Wine Cellars', 'Canada'),\n",
       " ('Lazy K Kountry Store', 'USA'),\n",
       " ('Lehmanns Marktstand', 'Germany'),\n",
       " (\"Let's Stop N Shop\", 'USA'),\n",
       " ('LILA-Supermercado', 'Venezuela'),\n",
       " ('LINO-Delicateses', 'Venezuela'),\n",
       " ('Lonesome Pine Restaurant', 'USA'),\n",
       " ('Magazzini Alimentari Riuniti', 'Italy'),\n",
       " ('Maison Dewey', 'Belgium'),\n",
       " ('Mere Paillarde', 'Canada'),\n",
       " ('Morgenstern Gesundkost', 'Germany'),\n",
       " ('North/South', 'UK'),\n",
       " ('Oceano Atlantico Ltda.', 'Argentina'),\n",
       " ('Old World Delicatessen', 'USA'),\n",
       " ('Ottilies Kaseladen', 'Germany'),\n",
       " ('Paris specialites', 'France'),\n",
       " ('Pericles Comidas clasicas', 'Mexico'),\n",
       " ('Piccolo und mehr', 'Austria'),\n",
       " ('Princesa Isabel Vinhoss', 'Portugal'),\n",
       " ('Que Delicia', 'Brazil'),\n",
       " ('Queen Cozinha', 'Brazil'),\n",
       " ('QUICK-Stop', 'Germany'),\n",
       " ('Rancho grande', 'Argentina'),\n",
       " ('Rattlesnake Canyon Grocery', 'USA'),\n",
       " ('Reggiani Caseifici', 'Italy'),\n",
       " ('Ricardo Adocicados', 'Brazil'),\n",
       " ('Richter Supermarkt', 'Switzerland'),\n",
       " ('Romero y tomillo', 'Spain'),\n",
       " ('Sante Gourmet', 'Norway'),\n",
       " ('Save-a-lot Markets', 'USA'),\n",
       " ('Seven Seas Imports', 'UK'),\n",
       " ('Simons bistro', 'Denmark'),\n",
       " ('Specialites du monde', 'France'),\n",
       " ('Split Rail Beer & Ale', 'USA'),\n",
       " ('Supremes delices', 'Belgium'),\n",
       " ('The Big Cheese', 'USA'),\n",
       " ('The Cracker Box', 'USA'),\n",
       " ('Toms Spezialitaten', 'Germany'),\n",
       " ('Tortuga Restaurante', 'Mexico'),\n",
       " ('Tradicao Hipermercados', 'Brazil'),\n",
       " (\"Trail's Head Gourmet Provisioners\", 'USA'),\n",
       " ('Vaffeljernet', 'Denmark'),\n",
       " ('Victuailles en stock', 'France'),\n",
       " ('Vins et alcools Chevalier', 'France'),\n",
       " ('Die Wandernde Kuh', 'Germany'),\n",
       " ('Wartian Herkku', 'Finland'),\n",
       " ('Wellington Importadora', 'Brazil'),\n",
       " ('White Clover Markets', 'USA'),\n",
       " ('Wilman Kala', 'Finland'),\n",
       " ('Wolski', 'Poland')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map 연산자를 이용해 콤마로 split 하고 튜플로 리턴하는 구문 작성\n",
    "cusPairs = customerRDD.map(lambda x : (x.split(',')[0] , x.split(',')[1] ))\n",
    "cusPairs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alfreds Futterkiste',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe2b00>),\n",
       " ('Ana Trujillo Emparedados y helados',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe2ba8>),\n",
       " ('Antonio Moreno Taqueria',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe29e8>),\n",
       " ('Around the Horn', <pyspark.resultiterable.ResultIterable at 0x15e35fe2208>),\n",
       " ('Berglunds snabbkop',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe2f98>),\n",
       " ('Blauer See Delikatessen',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe2b38>),\n",
       " ('Blondel pere et fils',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe2080>),\n",
       " ('Bolido Comidas preparadas',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe2048>),\n",
       " (\"Bon app'\", <pyspark.resultiterable.ResultIterable at 0x15e36034fd0>),\n",
       " ('Bottom-Dollar Marketse',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e36034e80>),\n",
       " (\"B's Beverages\", <pyspark.resultiterable.ResultIterable at 0x15e36034e10>),\n",
       " ('Cactus Comidas para llevar',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e360341d0>),\n",
       " ('Centro comercial Moctezuma',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e360340b8>),\n",
       " ('Chop-suey Chinese',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e36034128>),\n",
       " ('Comercio Mineiro',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe01d0>),\n",
       " ('Consolidated Holdings',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe2898>),\n",
       " ('Drachenblut Delikatessend',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0240>),\n",
       " ('Du monde entier', <pyspark.resultiterable.ResultIterable at 0x15e35fe0cf8>),\n",
       " ('Eastern Connection',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0710>),\n",
       " ('Ernst Handel', <pyspark.resultiterable.ResultIterable at 0x15e35fe0780>),\n",
       " ('Familia Arquibaldo',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0898>),\n",
       " ('FISSA Fabrica Inter. Salchichas S.A.',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0630>),\n",
       " ('Folies gourmandes',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe05c0>),\n",
       " ('Folk och fa HB', <pyspark.resultiterable.ResultIterable at 0x15e35fe0c50>),\n",
       " ('Frankenversand', <pyspark.resultiterable.ResultIterable at 0x15e35fe0d30>),\n",
       " ('France restauration',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0eb8>),\n",
       " ('Franchi S.p.A.', <pyspark.resultiterable.ResultIterable at 0x15e35fe0e10>),\n",
       " ('Furia Bacalhau e Frutos do Mar',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0fd0>),\n",
       " ('Galeria del gastronomo',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0ef0>),\n",
       " ('Godos Cocina Tipica',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0b70>),\n",
       " ('Gourmet Lanchonetes',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe08d0>),\n",
       " ('Great Lakes Food Market',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e36034f60>),\n",
       " ('GROSELLA-Restaurante',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0dd8>),\n",
       " ('Hanari Carnes', <pyspark.resultiterable.ResultIterable at 0x15e35fe0f60>),\n",
       " ('HILARIoN-Abastos',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe0cc0>),\n",
       " ('Hungry Coyote Import Store',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fe02b0>),\n",
       " ('Hungry Owl All-Night Grocers',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1cf8>),\n",
       " ('Island Trading', <pyspark.resultiterable.ResultIterable at 0x15e35ff1b38>),\n",
       " ('Koniglich Essen', <pyspark.resultiterable.ResultIterable at 0x15e35ff1f60>),\n",
       " (\"La corne d'abondance\",\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1390>),\n",
       " (\"La maison d'Asie\",\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1128>),\n",
       " ('Laughing Bacchus Wine Cellars',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1be0>),\n",
       " ('Lazy K Kountry Store',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1048>),\n",
       " ('Lehmanns Marktstand',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1908>),\n",
       " (\"Let's Stop N Shop\",\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1780>),\n",
       " ('LILA-Supermercado',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1748>),\n",
       " ('LINO-Delicateses',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1940>),\n",
       " ('Lonesome Pine Restaurant',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1828>),\n",
       " ('Magazzini Alimentari Riuniti',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff19b0>),\n",
       " ('Maison Dewey', <pyspark.resultiterable.ResultIterable at 0x15e35ff1eb8>),\n",
       " ('Mere Paillarde', <pyspark.resultiterable.ResultIterable at 0x15e35ff1fd0>),\n",
       " ('Morgenstern Gesundkost',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1160>),\n",
       " ('North/South', <pyspark.resultiterable.ResultIterable at 0x15e35ff1240>),\n",
       " ('Oceano Atlantico Ltda.',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff11d0>),\n",
       " ('Old World Delicatessen',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1e10>),\n",
       " ('Ottilies Kaseladen',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1dd8>),\n",
       " ('Paris specialites',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff12b0>),\n",
       " ('Pericles Comidas clasicas',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35ff1320>),\n",
       " ('Piccolo und mehr',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f61278>),\n",
       " ('Princesa Isabel Vinhoss',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f61d68>),\n",
       " ('Que Delicia', <pyspark.resultiterable.ResultIterable at 0x15e35f61748>),\n",
       " ('Queen Cozinha', <pyspark.resultiterable.ResultIterable at 0x15e35f61da0>),\n",
       " ('QUICK-Stop', <pyspark.resultiterable.ResultIterable at 0x15e35f61470>),\n",
       " ('Rancho grande', <pyspark.resultiterable.ResultIterable at 0x15e35fe0080>),\n",
       " ('Rattlesnake Canyon Grocery',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f61f60>),\n",
       " ('Reggiani Caseifici',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f61240>),\n",
       " ('Ricardo Adocicados',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f61710>),\n",
       " ('Richter Supermarkt',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f61f28>),\n",
       " ('Romero y tomillo',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f61860>),\n",
       " ('Sante Gourmet', <pyspark.resultiterable.ResultIterable at 0x15e35f7d2e8>),\n",
       " ('Save-a-lot Markets',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f7d0b8>),\n",
       " ('Seven Seas Imports',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f7d358>),\n",
       " ('Simons bistro', <pyspark.resultiterable.ResultIterable at 0x15e35f7de10>),\n",
       " ('Specialites du monde',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35f7d9b0>),\n",
       " ('Split Rail Beer & Ale',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e346d8048>),\n",
       " ('Supremes delices',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fbfba8>),\n",
       " ('The Big Cheese', <pyspark.resultiterable.ResultIterable at 0x15e35fbf9b0>),\n",
       " ('The Cracker Box', <pyspark.resultiterable.ResultIterable at 0x15e35fbfb70>),\n",
       " ('Toms Spezialitaten',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fbf1d0>),\n",
       " ('Tortuga Restaurante',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35fbf4e0>),\n",
       " ('Tradicao Hipermercados',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e35da9a20>),\n",
       " (\"Trail's Head Gourmet Provisioners\",\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e3603a5c0>),\n",
       " ('Vaffeljernet', <pyspark.resultiterable.ResultIterable at 0x15e3603a5f8>),\n",
       " ('Victuailles en stock',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e3603a828>),\n",
       " ('Vins et alcools Chevalier',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e3603a7b8>),\n",
       " ('Die Wandernde Kuh',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e3603a668>),\n",
       " ('Wartian Herkku', <pyspark.resultiterable.ResultIterable at 0x15e3603a6d8>),\n",
       " ('Wellington Importadora',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e3603a780>),\n",
       " ('White Clover Markets',\n",
       "  <pyspark.resultiterable.ResultIterable at 0x15e3603a898>),\n",
       " ('Wilman Kala', <pyspark.resultiterable.ResultIterable at 0x15e3603a978>),\n",
       " ('Wolski', <pyspark.resultiterable.ResultIterable at 0x15e3603a518>)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey() : 키값을 리스트 형태로 리턴하는 함수\n",
    "cusPairs.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'UK'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-f5920bb8933d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcountry\u001b[0m  \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcountry\u001b[0m  \u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroupKey\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m }\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcustomerDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UK'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'UK'"
     ]
    }
   ],
   "source": [
    "# UK에 사는 고객 이름만 출력한다면? (dict 형식으로 만들어)\n",
    "groupKey = cusPairs.groupByKey().collect()\n",
    "#groupKey\n",
    "\n",
    "for country , name in groupKey :\n",
    "    if country == 'UK' :\n",
    "        for name in names :\n",
    "            print(name)\n",
    "\n",
    "customerDict = {\n",
    "    country  : [name for name in names] for country  , names in groupKey\n",
    "}\n",
    "customerDict['UK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alfreds Futterkiste', 'Germany'),\n",
       " ('Ana Trujillo Emparedados y helados', 'Mexico'),\n",
       " ('Antonio Moreno Taqueria', 'Mexico'),\n",
       " ('Around the Horn', 'UK'),\n",
       " (\"B's Beverages\", 'UK'),\n",
       " ('Berglunds snabbkop', 'Sweden'),\n",
       " ('Blauer See Delikatessen', 'Germany'),\n",
       " ('Blondel pere et fils', 'France'),\n",
       " ('Bolido Comidas preparadas', 'Spain'),\n",
       " (\"Bon app'\", 'France')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sortByKey : Key 오름차순으로 정렬하고 상위 10개만 뽑는다면?\n",
    "cusPairs.sortByKey().collect()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RcusPairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-48c00a65c958>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 나라별 고객이 몇 명인지 카운트한다면?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# cusPairs.collect()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmapR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRcusPairs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmapR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m {\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RcusPairs' is not defined"
     ]
    }
   ],
   "source": [
    "# 나라별 고객이 몇 명인지 카운트한다면?\n",
    "# cusPairs.collect()\n",
    "mapR = RcusPairs.mapValues(lambda x : 1).reduceByKey(lambda x , y : x+y)\n",
    "type( mapR.collect())\n",
    "{\n",
    "    i : j\n",
    "    for i , j in mapR.collect()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spark dataframe 은 RDD 의 확장된 구조이다\n",
    "- 행 , 열로 이뤄진 내장 RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 생성\n",
    "- 스파크 세션을 이용한 생성\n",
    "- SQL 컨텍스트의 테이블을 통한 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf , SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x15e35fe0940>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlCtx = SQLContext(spark)\n",
    "sqlCtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\i\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(brand='Ford', models={'name': 'Fiesta', 'price': '14260'}),\n",
       " Row(brand='Ford', models={'name': 'Focus', 'price': '18825'}),\n",
       " Row(brand='Ford', models={'name': 'Mustang', 'price': '26670'}),\n",
       " Row(brand='BMW', models={'name': '320', 'price': '40250'}),\n",
       " Row(brand='BMW', models={'name': 'X3', 'price': '41000'}),\n",
       " Row(brand='BMW', models={'name': 'X5', 'price': '60700'}),\n",
       " Row(brand='Fiat', models={'name': '500', 'price': '16495'})]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json 파일\n",
    "# json -> RDD -> DataFrame\n",
    "sample_json = spark.textFile('C:/Users/i/TIL/Spark/exec_data/cars.json')\n",
    "cars_df = sqlCtx.createDataFrame(sample_json.map(lambda x : json.loads(x)))\n",
    "cars_df.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- models: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|brand|              models|\n",
      "+-----+--------------------+\n",
      "| Ford|[name -> Fiesta, ...|\n",
      "| Ford|[name -> Focus, p...|\n",
      "| Ford|[name -> Mustang,...|\n",
      "|  BMW|[name -> 320, pri...|\n",
      "|  BMW|[name -> X3, pric...|\n",
      "|  BMW|[name -> X5, pric...|\n",
      "| Fiat|[name -> 500, pri...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|brand|\n",
      "+-----+\n",
      "| Ford|\n",
      "| Ford|\n",
      "| Ford|\n",
      "|  BMW|\n",
      "|  BMW|\n",
      "|  BMW|\n",
      "| Fiat|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 프레임에 대한 연산\n",
    "# select()\n",
    "cars_df.select('brand').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|price|\n",
      "+-----+\n",
      "|14260|\n",
      "|18825|\n",
      "|26670|\n",
      "|40250|\n",
      "|41000|\n",
      "|60700|\n",
      "|16495|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cars_df.select('models.price').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컬럼의 타입 변환\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cars_price_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-c54e0b13d6a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcar_price_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcars_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'brand'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'models.name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'models.price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# car_price_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcar_price_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcars_price_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'price'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mcars_price_type\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIntegerType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cars_price_type' is not defined"
     ]
    }
   ],
   "source": [
    "car_price_type = cars_df.select('brand' , 'models.name', 'models.price')\n",
    "# car_price_type\n",
    "car_price_type = cars_price_type.withColumn('price' , cars_price_type['price'].cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cars_price_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-9312a97d65d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# cars_price_type.printSchema()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcars_price_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cars_price_type' is not defined"
     ]
    }
   ],
   "source": [
    "# cars_price_type.printSchema()\n",
    "cars_price_type.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cars_price_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-40f308aea688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 비교 연산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# cars_price_type.collect()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcars_price_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcars_price_type\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cars_price_type' is not defined"
     ]
    }
   ],
   "source": [
    "# 비교 연산\n",
    "# cars_price_type.collect()\n",
    "cars_price_type.filter(cars_price_type['price'] > 20000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cars_price_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-536d4f371bc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 그룹핑\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcars_price_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'brand'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cars_price_type' is not defined"
     ]
    }
   ],
   "source": [
    "# 그룹핑\n",
    "cars_price_type.groupBy('brand').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
